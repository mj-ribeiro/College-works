View(df)
rm(df)
w = runif(i*r, 0, 1)
tau_w =  runif(i*r, -1, 1)
tau_h = runif(i*r, -1, 1)
df = data.frame(tau_w, tau_h, w)
df[3]
df[1]
View(df)
View(df)
res = polyopt(x1, obj, gr=NULL)
res = solnp(x1,      #starting values
obj,   #function to optimise
LB=c(rep(0, length(x1))), #lower bound for parameters i.e. greater than zero
UB=c(rep(1, length(x1)))) #upper bound for parameters i.e less than one
obj(x1)
w_r = w_rf()
obj(x1)
res = solnp(x1,      #starting values
obj,   #function to optimise
LB=c(rep(0, length(x1))), #lower bound for parameters i.e. greater than zero
UB=c(rep(1, length(x1)))) #upper bound for parameters i.e less than one
res = optim(x1,      #starting values
obj)   #function to optimise
knitr::opts_chunk$set(echo = FALSE)
x = rnorm(1000, 0, 1)
hist(x, breaks = 25, col = 'lightgreen)
x = rnorm(1000, 0, 1)
hist(x, breaks = 25, col = 'lightgreen')
knitr::opts_chunk$set(echo = FALSE)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
knitr::opts_chunk$set(echo = FALSE)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
summary(reg)
seed(1)
set.seed(1)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
summary(reg)
knitr::opts_chunk$set(echo = FALSE)
set.seed(1)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
#summary(reg)
knitr::kable(reg)
View(reg)
set.seed(1)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
knitr::kable(summary(reg))
set.seed(1)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
summary(reg)
setwd("C:/Users/user/Downloads/ML_work/Algorithm")
df = read.csv('naive_base.csv')
df = read.csv('naive_base.csv')
head(df)
df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
head(df)
df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
knitr::kable(head(df))
df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
knitr::kable(head(df))
---
title: "Naive Bayes in R language"
author: "Marcos J Ribeiro"
date: "07/05/2020"
fontsize: 9pt
output:
beamer_presentation:
theme: "AnnArbor"
colortheme: "dolphin"
fonttheme: "structurebold"
---
title: "Naive Bayes in R language"
author: "Marcos J Ribeiro"
institute: 'FEARP-USP'
date: "07/05/2020"
fontsize: 8pt
output:
beamer_presentation:
theme: "AnnArbor"
colortheme: "dolphin"
fonttheme: "structurebold"
slide_level: 2
---
library(tensorflow)
library(tseries)
library(timeSeries)
library(forecast)   # auto.arima
library(quantmod)
library(fGarch)
library(rugarch)
#---- Get data
ibov = getSymbols('^BVSP', src='yahoo',
from= '1999-12-01',
to = '2020-04-01',
#periodicity = "monthly",    # IBOV mensal
auto.assign = F)[,4]
#----- returns
ret = diff(log(ibov))
colnames(ret) = c('ret')
ret = ret[is.na(ret)==F]  # Drop na to work
#---- GARCH Model
spec1 = ugarchspec(variance.model=list(model="fGARCH",
garchOrder=c(1,1), submodel='TGARCH'),
mean.model=list(armaOrder=c(0,0), include.mean=TRUE, archm=T),
distribution.model="norm")
garch2 = ugarchfit(spec = spec1, data= ret)
garch2
ts.plot(sigma(garch2))
plot(ret**2)
windows()
for(i in 1:(length(ret)-1)){
plot(as.vector( ret[1:(1+i)]), col='blue')
}
library(vars)
x = rnorm(1000)
y = rnorm(1000)
vmat = as.matrix(cbind(y, x))
vmat
head(vmat)
tail(vmat)
vfit = VAR(vmat)
summary(vfit)
irf(vfit)
plot(irf(vfit))
summary(vfit)
vfit
vfit = VAR(vmat, p=2)
summary(vfit)
curve(x)
curve(x)
curve(dnorm(x))
curve(dnorm(k, mean = 0, sd=1))
call(x)
N <- 40
x1 <- 10
x2 <- 20
b1 <- 100
b2 <- 10
mu <- 0
sig2e <- 2500
sde <- sqrt(sig2e)
yhat1 <- b1+b2*x1
yhat2 <- b1+b2*x2
curve(dnorm(x, mean=yhat1, sd=sde), 0, 500, col="blue")
curve(dnorm(k, mean=yhat1, sd=sde), 0, 500, col="blue")
rm(x)
N <- 40
x1 <- 10
x2 <- 20
b1 <- 100
b2 <- 10
mu <- 0
sig2e <- 2500
sde <- sqrt(sig2e)
yhat1 <- b1+b2*x1
yhat2 <- b1+b2*x2
curve(dnorm(x, mean=yhat1, sd=sde), 0, 500, col="blue")
curve((k))
curve((x))
curve((x^2))
curve((x^3))
curve((x^3), xlim = c(-1,1))
curve((x), xlim = c(-1,1))
plot(y)
curve((x), xlim = c(-1,1))
plot(rnorm(30), col='lightblue')
curve((x), xlim = c(-1,1), add=T)
curve((x), xlim = c(-1,1), add=T)
curve((x), xlim = c(-1,1))
plot(rnorm(30), col='lightblue')
plot(rnorm(30,0,1), col='lightblue')
plot(rnorm(30,0,1), col='lightblue')
curve((x), add=T)
plot(rnorm(30,0,1), col='lightblue')
curve((x^2), add=T)
curve((x+2), add=T)
curve((x+2X), add=T)
curve((x+2x), add=T)
curve((x+2*x), add=T)
curve((x+2*x), add=T)
plot(rnorm(30,0,1), col='lightblue')
curve((x+2*x), add=T)
curve((2*x+2*x), add=T)
curve((0.8*x+2*x), add=T)
curve((0.01x), add=T)
curve((0.01*x), add=T)
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='lightblue')
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray')
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19)
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19, size=3)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19)
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19)
curve((0.1*x), add=T, col='red')
curve(dnorm(x))
curve(dnorm(x), xlim=c(-1,1))
curve(dnorm(x), xlim=c(-3,3))
curve(dnorm(x), xlim=c(-4,4))
curve(dnorm(x), xlim=c(-6,6))
curve(dnorm(x), xlim=c(-8,8))
write.csv(ret, 'ret')
write.table(ret, 'ret')
write.xlsx(ret, 'ret')
library(xlsx)
write.xlsx(ret, 'ret')
write.xlsx(ret, 'c:/ret.xlsx')
write.xlsx(ret,"C:/Users/user/Documents/ret.xlsx")
spec2 = ugarchspec(variance.model=list(model="sGARCH",
garchOrder=c(1,1)),
mean.model=list(armaOrder=c(0,0), include.mean=TRUE),
distribution.model="norm")
garch3 = ugarchfit(spec = spec2, data= ret)
garch3
spec1 = ugarchspec(variance.model=list(model="fGARCH",
garchOrder=c(1,1), submodel='TGARCH'),
mean.model=list(armaOrder=c(0,0), include.mean=TRUE, archm=T),
distribution.model="norm")
garch2 = ugarchfit(spec = spec1, data= ret)
garch2
sd(ret)
library(sn)
curve(dsn(x))
curve(dsn(x), ylim=c(-8,8))
curve(dsn(x), ylim=c(0,8))
curve(dsn(x, xi = 2), ylim=c(0,8))
curve(dsn(x, xi = 2, omega = 0.2, alpha = 0.22), ylim=c(0,8))
curve(dsn(x, xi = 2, omega = 0.2, alpha = 0.22))
curve(dsn(x, xi = 22, omega = 0.2, alpha = 0.22))
curve(dsn(x, omega = 0.2, alpha = 0.22))
curve(dsn(x, omega = 3, alpha = 0.22))
curve(dsn(x, omega = 8, alpha = 0.22))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(0,8))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(-8,8))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(0,8))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(0,0.8))
curve(dsn(x, omega = 8, alpha = 0.22))
curve(dsn(x, omega = 0.11, alpha = 0.22))
curve(dsn(x, omega = 0.21, alpha = 0.22))
curve(dsn(x, omega = 0.41, alpha = 0.22))
curve(dsn(x, omega = 0.1, alpha = 0.22))
curve(dsn(x, omega = 0.1, alpha = 2))
curve(dsn(x, omega = 0.1, alpha = 10))
curve(dsn(x, omega = 0.1, alpha = 100))
curve(dsn(x, omega = 0.1, alpha = 0.011))
curve(dst(x, omega = 0.1, alpha = 0.011))
curve(dst(x, omega = 1, alpha = 0.011))
curve(dst(x, omega = 1, alpha = 0.11))
curve(dst(x, omega = 1, alpha = 11))
curve(dst(x, omega = 0.9, alpha = 11))
curve(dst(x, omega = 0.11, alpha = 11))
curve(dst(x, omega = 0.11, alpha = 0.11))
rst(x, omega = 0.11, alpha = 0.11))
rst(100, omega = 0.11, alpha = 0.11))
rst(100, omega = 0.11, alpha = 0.11)
rst(100, omega = 0.2, alpha = 0.11, xi = 2)
rst(100, omega = 0.2, alpha = 0.11, xi = 0)
rst(100, omega = 0.2, alpha = 0.11, xi = 0, nu = 2)
sin(30)
sin(30Â°)
sin(30)
sin(pi/6)
tan(pi/6)
curve(sin(x))
curve(sin(x), xlim = c(-3,3))
curve(sin(x), xlim = c(-30,30))
curve(sin(x), xlim = c(-10,10))
line(h=0)
aline(h=0)
abline(h=0)
curve(tan(x), xlim = c(-10,10))
curve(tan(x), xlim = c(-10,10))
abline(h=0)
curve((x), xlim = c(-10,10))
abline(v=0)
curve((x), xlim = c(0,10))
abline(v=0)
abline(v=10)
curve((x), xlim = c(0,10))
abline(v=10)
atan(1)
atan(pi)
pi/4
tan(pi/3)
tan(2*pi/3)
fd = function(x, alpha){
disf = exp(-x^alpha)
}
fd(2, 1)
fd = function(x, alpha){
disf = exp(-x^alpha)
return(disf)
}
fd(2, 1)
fd(0.2, 1)
curve(fd(x, 1))
curve(fd(x, 1), xlim = c(-10,10))
fd = function(x, alpha){
disf = exp(-x^(-alpha))
return(disf)
}
curve(fd(x, 1), xlim = c(-10,10))
curve(fd(x, 1), xlim = c(0,10))
curve(fd(x, 1), xlim = c(0,4))
install.packages('Rtolls40')
install.packages('Rtolls')
library(installr)
updateR()
updateR()
install.packages('Rtolls')
install.packages(c("fGarch", "forecast", "installr", "quantmod", "rugarch", "timeSeries", "tseries", "vars", "xlsx"))
install.packages('Rtolls')
install.packages('Rtolls40')
install.packages('rtolls40')
curve(x^0.4)
curve(x^0.4, ylim = c(0, 4))
curve(x^0.4, ylim = c(0, 1))
curve(x^0.2, ylim = c(0, 1))
curve(x^0.1, ylim = c(0, 1))
curve(x^0.01, ylim = c(0, 1))
curve(x^0.001, ylim = c(0, 1))
curve(x^0.1, ylim = c(0, 1))
curve(x^1, ylim = c(0, 1))
curve(x^0.1, ylim = c(0, 1))
curve(x^0.2, ylim = c(0, 1))
curve(x^0.4, ylim = c(0, 1), add=T)
install.packages("knitr")
install.packages("readxl")
# Defining my work diretory
setwd("C:/Users/user/Downloads/ML_work/Algorithm")
library(readxl)
teste <- read_excel("teste.xlsx")
teste$foot = NULL
df2 = teste[,'sex']
teste[,'sex']=NULL
teste$sex = df2
teste = data.frame(teste)
#--- function
naive_marcos2 = function(k, df){
df = as.data.frame(df)
#fator =  factor(df[,k])
a = prop.table(table(df[ ,k]))
ta = length(a)
nm = rownames(a)
print('Marcos Naive Bayes Classifier for Discrete Predictors')
cat('A-priori probabilities:\n')
#df2 = df[ , k]
print(a)
#df[ ,k] = NULL
#col_n = colnames(df)
#df[,k] = df2
M = array(0, dim = c(2,2, ta))
m = matrix(0, 2, 2)
for(g in 1:ta){
m1 = as.matrix(tapply(df[,1], df[,k], mean)[g])
v1 = as.matrix(tapply(df[,1], df[,k], sd)[g])
m2 = tapply(df[,2], df[,k], mean)[g]
v2 = tapply(df[,2], df[,k], sd)[g]
m = matrix(c(m1, m2, v1, v2)  )
M[, ,g] = m
cat(nm[g], '\n')
print(M[, ,g])
}
return(M)
}
cc = naive_marcos2('sex', teste)
library(readxl)
teste <- read_excel("teste.xlsx")
setwd("C:/Users/user/Downloads/ML_work/Algorithm")
getwd()
# Defining my work diretory
setwd("D:/Git projects/college_works/ML_1")
# import data
df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
naive_marcos = function(k, df){
a = prop.table(table(df[ ,k]))
ta = length(a)
nm = rownames(a)
print(strrep('=-', 30))
print('Marcos Naive Bayes Classifier for Discrete Predictors')
print(strrep('=-', 30))
cat('A-priori probabilities:\n')
print(a)
# simple trick to get names of variables without class variable
df2 = df[ , k]
df[ , k] = NULL
col_n = colnames(df)
df$k = df2
# defining lengths
t_b1 =length( prop.table(table(df[ ,col_n[1]])))
t_c1 =length( prop.table(table(df[ ,col_n[2]])))
# probabilities array
M1 = array(0, dim=c(t_b1, t_c1, ta) )
for (k in 1:ta) {
f1 = df['k'] == nm[k]
b1 = prop.table(table(df[f1,col_n[1]]))
c1 = prop.table(table(df[f1,col_n[2]]))
for(j in 1:length(c1)){
for ( i in 1:length(b1) ) {
M1[i, j, k] = a[k]*b1[i]*c1[j]
}
}
}
dimnames(M1) = list(rownames(prop.table(table(df[ ,col_n[1]]))),rownames( prop.table(table(df[ ,col_n[2]]))), nm )
cat('Conditional Probabilities: \n')
return(M1)
}
cl = naive_marcos('risco', df)
cl
pred_marcos = function(k, df, df_n, cl, cclas=0){
# k -> class
# df -> data to training algorithm
# df_c -> new data vectors
# cclas -> to get classification (1) or probabilities (0)
# cl -> classifier
a = prop.table(table(df[,k]))
ta = length(a)
nm = rownames(a)
tvv = length(df_n[,1])
cnewd = c()
pmax = c()
v = matrix(0, tvv, ta)
for(i in 1:tvv){
v[i, ] =  cl[df_c[i, 1], df_n[i, 2],  ]
v[i, ] = v[i, ]/sum(v[i, ])
pmax[i] = which.max(v[i, ])
cnewd[i] = nm[pmax[i]]
}
colnames(v) = nm
if(cclas==0){
return(v)
}else{
return(cnewd)
}
}
historia = c('boa',    'boa',  'ruim', 'ruim', 'desconhecida', 'desconhecida')
divida = c('baixa', 'alta', 'baixa', 'alta',    'baixa', 'alta')
df_teste = data.frame(historia, divida)
d = pred_marcos('risco',  df, df_teste, cl, cclas = 1)
pred_marcos = function(k, df, df_n, cl, cclas=0){
# k -> class
# df -> data to training algorithm
# df_c -> new data vectors
# cclas -> to get classification (1) or probabilities (0)
# cl -> classifier
a = prop.table(table(df[,k]))
ta = length(a)
nm = rownames(a)
tvv = length(df_n[,1])
cnewd = c()
pmax = c()
v = matrix(0, tvv, ta)
for(i in 1:tvv){
v[i, ] =  cl[df_n[i, 1], df_n[i, 2],  ]
v[i, ] = v[i, ]/sum(v[i, ])
pmax[i] = which.max(v[i, ])
cnewd[i] = nm[pmax[i]]
}
colnames(v) = nm
if(cclas==0){
return(v)
}else{
return(cnewd)
}
}
historia = c('boa',    'boa',  'ruim', 'ruim', 'desconhecida', 'desconhecida')
divida = c('baixa', 'alta', 'baixa', 'alta',    'baixa', 'alta')
df_teste = data.frame(historia, divida)
d = pred_marcos('risco',  df, df_teste, cl, cclas = 1)
d
knitr::opts_chunk$set(echo = TRUE)
source("D:/Git projects/college_works/ML_1/Nbayes2_work.R")
source("D:/Git projects/college_works/ML_1/Nbayes2_work.R")
setwd("D:/Git projects/college_works/ML_1")
df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
historia = c('boa',    'boa',  'ruim', 'ruim', 'desconhecida', 'desconhecida')
divida = c('baixa', 'alta', 'baixa', 'alta',    'baixa', 'alta')
df_teste = data.frame(historia, divida)
