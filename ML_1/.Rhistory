curve((x), xlim = c(-1,1))
plot(y)
curve((x), xlim = c(-1,1))
plot(rnorm(30), col='lightblue')
curve((x), xlim = c(-1,1), add=T)
curve((x), xlim = c(-1,1), add=T)
curve((x), xlim = c(-1,1))
plot(rnorm(30), col='lightblue')
plot(rnorm(30,0,1), col='lightblue')
plot(rnorm(30,0,1), col='lightblue')
curve((x), add=T)
plot(rnorm(30,0,1), col='lightblue')
curve((x^2), add=T)
curve((x+2), add=T)
curve((x+2X), add=T)
curve((x+2x), add=T)
curve((x+2*x), add=T)
curve((x+2*x), add=T)
plot(rnorm(30,0,1), col='lightblue')
curve((x+2*x), add=T)
curve((2*x+2*x), add=T)
curve((0.8*x+2*x), add=T)
curve((0.01x), add=T)
curve((0.01*x), add=T)
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='lightblue')
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray')
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19)
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19, size=3)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19)
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19)
curve((0.1*x), add=T, col='red')
curve(dnorm(x))
curve(dnorm(x), xlim=c(-1,1))
curve(dnorm(x), xlim=c(-3,3))
curve(dnorm(x), xlim=c(-4,4))
curve(dnorm(x), xlim=c(-6,6))
curve(dnorm(x), xlim=c(-8,8))
write.csv(ret, 'ret')
write.table(ret, 'ret')
write.xlsx(ret, 'ret')
library(xlsx)
write.xlsx(ret, 'ret')
write.xlsx(ret, 'c:/ret.xlsx')
write.xlsx(ret,"C:/Users/user/Documents/ret.xlsx")
spec2 = ugarchspec(variance.model=list(model="sGARCH",
garchOrder=c(1,1)),
mean.model=list(armaOrder=c(0,0), include.mean=TRUE),
distribution.model="norm")
garch3 = ugarchfit(spec = spec2, data= ret)
garch3
spec1 = ugarchspec(variance.model=list(model="fGARCH",
garchOrder=c(1,1), submodel='TGARCH'),
mean.model=list(armaOrder=c(0,0), include.mean=TRUE, archm=T),
distribution.model="norm")
garch2 = ugarchfit(spec = spec1, data= ret)
garch2
sd(ret)
library(sn)
curve(dsn(x))
curve(dsn(x), ylim=c(-8,8))
curve(dsn(x), ylim=c(0,8))
curve(dsn(x, xi = 2), ylim=c(0,8))
curve(dsn(x, xi = 2, omega = 0.2, alpha = 0.22), ylim=c(0,8))
curve(dsn(x, xi = 2, omega = 0.2, alpha = 0.22))
curve(dsn(x, xi = 22, omega = 0.2, alpha = 0.22))
curve(dsn(x, omega = 0.2, alpha = 0.22))
curve(dsn(x, omega = 3, alpha = 0.22))
curve(dsn(x, omega = 8, alpha = 0.22))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(0,8))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(-8,8))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(0,8))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(0,0.8))
curve(dsn(x, omega = 8, alpha = 0.22))
curve(dsn(x, omega = 0.11, alpha = 0.22))
curve(dsn(x, omega = 0.21, alpha = 0.22))
curve(dsn(x, omega = 0.41, alpha = 0.22))
curve(dsn(x, omega = 0.1, alpha = 0.22))
curve(dsn(x, omega = 0.1, alpha = 2))
curve(dsn(x, omega = 0.1, alpha = 10))
curve(dsn(x, omega = 0.1, alpha = 100))
curve(dsn(x, omega = 0.1, alpha = 0.011))
curve(dst(x, omega = 0.1, alpha = 0.011))
curve(dst(x, omega = 1, alpha = 0.011))
curve(dst(x, omega = 1, alpha = 0.11))
curve(dst(x, omega = 1, alpha = 11))
curve(dst(x, omega = 0.9, alpha = 11))
curve(dst(x, omega = 0.11, alpha = 11))
curve(dst(x, omega = 0.11, alpha = 0.11))
rst(x, omega = 0.11, alpha = 0.11))
rst(100, omega = 0.11, alpha = 0.11))
rst(100, omega = 0.11, alpha = 0.11)
rst(100, omega = 0.2, alpha = 0.11, xi = 2)
rst(100, omega = 0.2, alpha = 0.11, xi = 0)
rst(100, omega = 0.2, alpha = 0.11, xi = 0, nu = 2)
sin(30)
sin(30Â°)
sin(30)
sin(pi/6)
tan(pi/6)
curve(sin(x))
curve(sin(x), xlim = c(-3,3))
curve(sin(x), xlim = c(-30,30))
curve(sin(x), xlim = c(-10,10))
line(h=0)
aline(h=0)
abline(h=0)
curve(tan(x), xlim = c(-10,10))
curve(tan(x), xlim = c(-10,10))
abline(h=0)
curve((x), xlim = c(-10,10))
abline(v=0)
curve((x), xlim = c(0,10))
abline(v=0)
abline(v=10)
curve((x), xlim = c(0,10))
abline(v=10)
atan(1)
atan(pi)
pi/4
tan(pi/3)
tan(2*pi/3)
fd = function(x, alpha){
disf = exp(-x^alpha)
}
fd(2, 1)
fd = function(x, alpha){
disf = exp(-x^alpha)
return(disf)
}
fd(2, 1)
fd(0.2, 1)
curve(fd(x, 1))
curve(fd(x, 1), xlim = c(-10,10))
fd = function(x, alpha){
disf = exp(-x^(-alpha))
return(disf)
}
curve(fd(x, 1), xlim = c(-10,10))
curve(fd(x, 1), xlim = c(0,10))
curve(fd(x, 1), xlim = c(0,4))
install.packages('Rtolls40')
install.packages('Rtolls')
library(installr)
updateR()
updateR()
install.packages('Rtolls')
install.packages(c("fGarch", "forecast", "installr", "quantmod", "rugarch", "timeSeries", "tseries", "vars", "xlsx"))
install.packages('Rtolls')
install.packages('Rtolls40')
install.packages('rtolls40')
curve(x^0.4)
curve(x^0.4, ylim = c(0, 4))
curve(x^0.4, ylim = c(0, 1))
curve(x^0.2, ylim = c(0, 1))
curve(x^0.1, ylim = c(0, 1))
curve(x^0.01, ylim = c(0, 1))
curve(x^0.001, ylim = c(0, 1))
curve(x^0.1, ylim = c(0, 1))
curve(x^1, ylim = c(0, 1))
curve(x^0.1, ylim = c(0, 1))
curve(x^0.2, ylim = c(0, 1))
curve(x^0.4, ylim = c(0, 1), add=T)
install.packages("knitr")
install.packages("readxl")
# Defining my work diretory
setwd("C:/Users/user/Downloads/ML_work/Algorithm")
library(readxl)
teste <- read_excel("teste.xlsx")
teste$foot = NULL
df2 = teste[,'sex']
teste[,'sex']=NULL
teste$sex = df2
teste = data.frame(teste)
#--- function
naive_marcos2 = function(k, df){
df = as.data.frame(df)
#fator =  factor(df[,k])
a = prop.table(table(df[ ,k]))
ta = length(a)
nm = rownames(a)
print('Marcos Naive Bayes Classifier for Discrete Predictors')
cat('A-priori probabilities:\n')
#df2 = df[ , k]
print(a)
#df[ ,k] = NULL
#col_n = colnames(df)
#df[,k] = df2
M = array(0, dim = c(2,2, ta))
m = matrix(0, 2, 2)
for(g in 1:ta){
m1 = as.matrix(tapply(df[,1], df[,k], mean)[g])
v1 = as.matrix(tapply(df[,1], df[,k], sd)[g])
m2 = tapply(df[,2], df[,k], mean)[g]
v2 = tapply(df[,2], df[,k], sd)[g]
m = matrix(c(m1, m2, v1, v2)  )
M[, ,g] = m
cat(nm[g], '\n')
print(M[, ,g])
}
return(M)
}
cc = naive_marcos2('sex', teste)
library(readxl)
teste <- read_excel("teste.xlsx")
setwd("C:/Users/user/Downloads/ML_work/Algorithm")
getwd()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "D:/Git projects/college_works/ML_1")
library(knitr)
library(kableExtra)
#options(kableExtra.latex.load_packages = FALSE)
library(magrittr)
knitr:: kable(a, booktabs = T) %>%kable_styling(full_width = T) %>%
column_spec(1, width = "8cm")
library(magrittr)
knitr::kable(a)
library(tinytex)
remove.packages("tinytex", lib="~/R/win-library/4.0")
install.packages('tinytex')
setwd("D:/Git projects/college_works/ML_1")
find = readRDS('findata.rds')
keep = c('x', 'oil', 'pca')
find2 = find[,keep]
library(ROSE)
tst = find2[232:241, ]
find2 = find2[1:231, ]
find2 = ovun.sample(as.factor(x)~., data=find2, method="both", p=0.5,
subset=options("subset")$subset,
na.action=options("na.action")$na.action, seed=1)
find2 = find2$data
tr = find2
tr = as.data.frame(tr)
##
keep2 = c('x', 'cb', 'vix')
find3 = find[,keep2]
tst2 = find3[232:241, ]
find3 = find3[1:231, ]
find3 = ovun.sample(as.factor(x)~., data=find3, method="both", p=0.5,
subset=options("subset")$subset,
na.action=options("na.action")$na.action, seed=1)
find3 = find3$data
tr2 = find3
tr2 = as.data.frame(tr2)
df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
teste <- read.csv("teste.csv")
teste$foot = NULL
df2 = teste[,'sex']
teste[,'sex']=NULL
teste$sex = df2
teste = data.frame(teste)
library(e1071) # library to work with naive bayes
clas2 = naiveBayes(x=df[-3], y = df$risco)
print(clas2)
decisionplot(clas2, x, class = "risco", main = "naive Bayes")
decisionplot <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
### guess how to get class labels from predict
### (unfortunately not very consistent between models)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
decisionplot(clas2, x, class = "risco", main = "naive Bayes")
decisionplot(clas2, df[-3], class = "risco", main = "naive Bayes")
decisionplot(clas2, df[,-3], class = "risco", main = "naive Bayes")
plot(df[,c(1,2)], df[3])
plot(df[,c(1,2)], col=df[3])
plot(df[,1:2], col=df[3])
plot(df[,1], df[,2], col = df[3])
plot(df[,1], df[,2])
plot(df$divida, df$historia)
View(df)
clas3 = naiveBayes(x=teste[-3], y = teste$sex)
prev3 = predict(clas3, newdata = dfn, 'raw')
height = c(5.4, 5.8, 6, 5)
weight = c(170, 183, 188, 188)
dfn = data.frame(height, weight)
clas3 = naiveBayes(x=teste[-3], y = teste$sex)
prev3 = predict(clas3, newdata = dfn, 'raw')
print(prev3)
View(teste)
decisionplot(clas3, df[,-3], class = "sex", main = "naive Bayes")
scaling(teste$sex)
scaling(teste$sex)
scaling(teste$sex)
scaling(teste$sex)
scaling(teste$sex)
scale(teste$sex)
teste$sex
ifelse(teste$sex=='male', 1, 0)
teste$sex = ifelse(teste$sex=='male', 1, 0)
plot(teste[,1:2], col=teste$sex)
plot(teste[,1:2])
plot(teste[,1:2], col=teste$sex)
plot(teste[,1:2], col=teste[,3])
teste$sex = as.factor(teste$sex)
plot(teste[,1:2], col=teste[,3])
plot(teste[,1:2], col=teste[,3], size=5)
plot(teste[,1:2], col=teste[,3], pch=19)
decisionplot(clas3, teste[,-3], class = "sex", main = "naive Bayes")
decisionplot(clas3, teste, class = "sex", main = "naive Bayes")
decisionplot(clas3, teste, class = "sex", main = "naive Bayes")
decisionplot(clas3, teste, class = "sex", main = "naive Bayes")
decisionplot <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
### guess how to get class labels from predict
### (unfortunately not very consistent between models)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
decisionplot <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
### guess how to get class labels from predict
### (unfortunately not very consistent between models)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
decisionplot(clas3, teste, class = "sex", main = "naive Bayes")
clas3 = naiveBayes(x=tr[-3], y = tr$x)
prev3 = predict(clas3, newdata = tst, 'raw')
print(prev3)
decisionplot(clas3, tr, class = "sex", main = "naive Bayes")
decisionplot(clas3, tr, class = "sex", main = "naive Bayes")
decisionplot(clas3, tr, class = "x", main = "naive Bayes")
decisionplot(clas3, tr, class = "x", main = "naive Bayes")
clas3 = naiveBayes(x=tr[-3], y = tr$x)
View(tr)
clas3 = naiveBayes(x=tr[-1], y = tr$x)
prev3 = predict(clas3, newdata = tst, 'raw')
print(prev3)
decisionplot(clas3, tr, class = "x", main = "naive Bayes")
print(prev3)
prev3 = predict(clas3, newdata = tst)
print(prev3)
prev3
prev3 = predict(clas3, newdata = tst, 'raw')
tr[ 1]= as.factor(tr[1])
clas3 = naiveBayes(x=tr[-1], y = tr$x)
prev3 = predict(clas3, newdata = tst, 'raw')
prev3 = predict(clas3, newdata = tst, 'raw')
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "D:/Git projects/college_works/ML_1")
source("D:/Git projects/college_works/ML_1/Nbayes2_work.R")
naivef = function(k, df, cd=1){
if(cd == 1){
naive_marcos(k, df)
}else if (cd == 0){
naive_marcos2(k, df)
}else{
cat('Type cd = 1 for categorical dependent variables, \n
and cd = 0 for non-categorical dependent variables.')
}}
predf = function(k, df, df_n, cl, cclas=0, cd=1){
if(cd == 1){
pred_marcos(k, df, df_n, cl, cclas)
}else if (cd == 0){
pred_marcos2(k, df, df_n, cl, cclas)
}else{
cat('Type cd = 1 for categorical dependent variables,
\n and cd = 0 for non-categorical dependent variables.')
}}
#df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
knitr::kable(head(df), caption = 'Dataset with categorical independent variables', label = 'Elaborate by author.')
library(ggplot2)
ggplot(data=df, aes(x=divida, y=historia, colour=risco)) + geom_point(size=5)+
geom_abline(xintercept=2, slope = 1.2)
cl = naivef('risco', df, cd=1)
head(cl[, ,1])
knitr::kable(head(df_teste), caption = 'New data set with categorical independent variables', label = 'Elaborate by author.')
predf('risco', df, df_teste, cl, cclas = 0, cd=1)
predf('risco', df, df_teste, cl, cclas = 1, cd=1)
library(e1071)
clas2 = naiveBayes(x=df[-3], y = as.factor(df$risco))
prev2 = predict(clas2, newdata = df_teste)
print(prev2)
knitr::kable(head(teste), caption = 'Data set with non categorical independent variables', label = 'Elaborate by author.')
library(ggplot2)
ggplot(data=teste, aes(x=weight, y=height, colour=sex)) + geom_point(size=5)+
geom_vline (xintercept =  160) + geom_hline (yintercept =  5.55)
cl2 = naivef('sex', teste, cd=0)
cl2
knitr::kable(head(dfn), caption = 'New data set with non categorical independent variables', label = 'Elaborate by author.')
predf('sex',teste, dfn, cl2, cclas =1, cd=0)
predf('sex',teste, dfn, cl2, cclas =0, cd=0)
library(e1071)
clas3 = naiveBayes(x=teste[-3], y = teste$sex)
prev3 = predict(clas3, newdata = dfn, 'raw')
print(prev3)
knitr::kable(head(census), caption = 'Census dataset head', label = 'Elaborate by author.')
knitr::kable(a)
library(ggplot2)
ggplot(data=census, aes(x=education, y=occupation, colour=income)) + geom_point(size=3.8)+
theme(axis.text.x = element_text(angle = 90)) + geom_vline(xintercept = 8.5)
tr1 = census[1:28000, ]
tst1 = census[28001:30162, ]
cl4 =  naivef('income', tr1, cd=1)
head(predf('income', tr1, tst1, cl4, cclas=0, cd=1))
ndp = predf('income', tr1, tst1, cl4, cclas=1, cd=1)
head(ndp)
acurracy1 = (sum((ndp==tst1[,'income'])*1)/length(tst1[,1])) *100
acurracy1
CMAX = function(w, n, s){
l = matrix(nrow=n,ncol = (w+1))
max = matrix(nrow=n, ncol = 1)
cmax = matrix(nrow=n, ncol = 1)
for (j in 1:n){
l[j, 1:(w+1)] = s[j:(w+j)]
max[j] = max(l[j, 1:(w+1)])
cmax[j] = l[j, (w+1)]/max(max[j])
}
return(cmax)
}
library(quantmod)
ibov = getSymbols('^BVSP', src='yahoo',
from= '2000-03-01',
to = '2020-04-01',
periodicity = "monthly",
auto.assign = F)[,4]
par(mfrow=c(1,2))
plot(index(ibov), as.vector(ibov) , main='Evolution\n of Ibovespa \n(a)', type='l', ylab='Ibov', xlab='Time')
hist(find$ret, col='lightgreen', probability = T, main = 'Histogram of Ibovespa\n returns \n(b)',
xlab = 'returns', breaks=35 )
abline(v =quantile(find$ret, 0.05))
library(ggplot2)
#ggplot(data=find2, aes(x=oil, y=pca, colour=as.factor(x))) + geom_point(size=4)
cl3 = naivef('x',tr, cd=0)
predf('x', tr, tst, cl3, cclas=0, cd=0)
predf('x', tr, tst, cl3, cclas=1, cd=0)
prev = predf('x', tr, tst, cl3, cclas=1, cd=0)
accuracy = (sum((prev == tst[,1])*1)/length(tst[,1]) )*100
accuracy
library(ggplot2)
ggplot(data=find3, aes(x=cb, y=vix, colour=as.factor(x))) + geom_point(size=4)+
geom_vline(xintercept = 104) + geom_hline(yintercept = 36)
cl4 = naivef('x',tr2, cd=0)
predf('x', tr2, tst2, cl4, cclas=0, cd=0)
predf('x', tr2, tst2, cl4, cclas=1, cd=0)
prev2 = predf('x', tr2, tst2, cl4, cclas=1, cd=0)
accuracy = (sum((prev2 == tst2[,1])*1)/length(tst2[,1]) )*100
accuracy
library(ElemStatLearn)
set = tst2
X1 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
X2 = seq(min(set[, 3]) - 1, max(set[, 3]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('vix', 'cb')
y_grid = prev2
plot(set[, -1], main = 'Naive Bayes (Test set)',
xlab = 'vix', ylab = 'cb',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
