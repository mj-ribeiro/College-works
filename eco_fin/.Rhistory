x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
summary(reg)
knitr::opts_chunk$set(echo = FALSE)
set.seed(1)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
#summary(reg)
knitr::kable(reg)
View(reg)
set.seed(1)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
knitr::kable(summary(reg))
set.seed(1)
x = rnorm(1000)
y = rnorm(1000)
reg = lm(x~y)
summary(reg)
setwd("C:/Users/user/Downloads/ML_work/Algorithm")
df = read.csv('naive_base.csv')
df = read.csv('naive_base.csv')
head(df)
df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
head(df)
df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
knitr::kable(head(df))
df = read.csv('naive_base.csv')
df$garantias =NULL
df$renda =NULL
knitr::kable(head(df))
---
title: "Naive Bayes in R language"
author: "Marcos J Ribeiro"
date: "07/05/2020"
fontsize: 9pt
output:
beamer_presentation:
theme: "AnnArbor"
colortheme: "dolphin"
fonttheme: "structurebold"
---
title: "Naive Bayes in R language"
author: "Marcos J Ribeiro"
institute: 'FEARP-USP'
date: "07/05/2020"
fontsize: 8pt
output:
beamer_presentation:
theme: "AnnArbor"
colortheme: "dolphin"
fonttheme: "structurebold"
slide_level: 2
---
library(tensorflow)
library(tseries)
library(timeSeries)
library(forecast)   # auto.arima
library(quantmod)
library(fGarch)
library(rugarch)
#---- Get data
ibov = getSymbols('^BVSP', src='yahoo',
from= '1999-12-01',
to = '2020-04-01',
#periodicity = "monthly",    # IBOV mensal
auto.assign = F)[,4]
#----- returns
ret = diff(log(ibov))
colnames(ret) = c('ret')
ret = ret[is.na(ret)==F]  # Drop na to work
#---- GARCH Model
spec1 = ugarchspec(variance.model=list(model="fGARCH",
garchOrder=c(1,1), submodel='TGARCH'),
mean.model=list(armaOrder=c(0,0), include.mean=TRUE, archm=T),
distribution.model="norm")
garch2 = ugarchfit(spec = spec1, data= ret)
garch2
ts.plot(sigma(garch2))
plot(ret**2)
windows()
for(i in 1:(length(ret)-1)){
plot(as.vector( ret[1:(1+i)]), col='blue')
}
library(vars)
x = rnorm(1000)
y = rnorm(1000)
vmat = as.matrix(cbind(y, x))
vmat
head(vmat)
tail(vmat)
vfit = VAR(vmat)
summary(vfit)
irf(vfit)
plot(irf(vfit))
summary(vfit)
vfit
vfit = VAR(vmat, p=2)
summary(vfit)
curve(x)
curve(x)
curve(dnorm(x))
curve(dnorm(k, mean = 0, sd=1))
call(x)
N <- 40
x1 <- 10
x2 <- 20
b1 <- 100
b2 <- 10
mu <- 0
sig2e <- 2500
sde <- sqrt(sig2e)
yhat1 <- b1+b2*x1
yhat2 <- b1+b2*x2
curve(dnorm(x, mean=yhat1, sd=sde), 0, 500, col="blue")
curve(dnorm(k, mean=yhat1, sd=sde), 0, 500, col="blue")
rm(x)
N <- 40
x1 <- 10
x2 <- 20
b1 <- 100
b2 <- 10
mu <- 0
sig2e <- 2500
sde <- sqrt(sig2e)
yhat1 <- b1+b2*x1
yhat2 <- b1+b2*x2
curve(dnorm(x, mean=yhat1, sd=sde), 0, 500, col="blue")
curve((k))
curve((x))
curve((x^2))
curve((x^3))
curve((x^3), xlim = c(-1,1))
curve((x), xlim = c(-1,1))
plot(y)
curve((x), xlim = c(-1,1))
plot(rnorm(30), col='lightblue')
curve((x), xlim = c(-1,1), add=T)
curve((x), xlim = c(-1,1), add=T)
curve((x), xlim = c(-1,1))
plot(rnorm(30), col='lightblue')
plot(rnorm(30,0,1), col='lightblue')
plot(rnorm(30,0,1), col='lightblue')
curve((x), add=T)
plot(rnorm(30,0,1), col='lightblue')
curve((x^2), add=T)
curve((x+2), add=T)
curve((x+2X), add=T)
curve((x+2x), add=T)
curve((x+2*x), add=T)
curve((x+2*x), add=T)
plot(rnorm(30,0,1), col='lightblue')
curve((x+2*x), add=T)
curve((2*x+2*x), add=T)
curve((0.8*x+2*x), add=T)
curve((0.01x), add=T)
curve((0.01*x), add=T)
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='lightblue')
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray')
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19)
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19, size=3)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19)
curve((0.1*x), add=T)
plot(abs(rnorm(30,0,1)), col='gray', lwd=19)
curve((0.1*x), add=T, col='red')
curve(dnorm(x))
curve(dnorm(x), xlim=c(-1,1))
curve(dnorm(x), xlim=c(-3,3))
curve(dnorm(x), xlim=c(-4,4))
curve(dnorm(x), xlim=c(-6,6))
curve(dnorm(x), xlim=c(-8,8))
write.csv(ret, 'ret')
write.table(ret, 'ret')
write.xlsx(ret, 'ret')
library(xlsx)
write.xlsx(ret, 'ret')
write.xlsx(ret, 'c:/ret.xlsx')
write.xlsx(ret,"C:/Users/user/Documents/ret.xlsx")
spec2 = ugarchspec(variance.model=list(model="sGARCH",
garchOrder=c(1,1)),
mean.model=list(armaOrder=c(0,0), include.mean=TRUE),
distribution.model="norm")
garch3 = ugarchfit(spec = spec2, data= ret)
garch3
spec1 = ugarchspec(variance.model=list(model="fGARCH",
garchOrder=c(1,1), submodel='TGARCH'),
mean.model=list(armaOrder=c(0,0), include.mean=TRUE, archm=T),
distribution.model="norm")
garch2 = ugarchfit(spec = spec1, data= ret)
garch2
sd(ret)
library(sn)
curve(dsn(x))
curve(dsn(x), ylim=c(-8,8))
curve(dsn(x), ylim=c(0,8))
curve(dsn(x, xi = 2), ylim=c(0,8))
curve(dsn(x, xi = 2, omega = 0.2, alpha = 0.22), ylim=c(0,8))
curve(dsn(x, xi = 2, omega = 0.2, alpha = 0.22))
curve(dsn(x, xi = 22, omega = 0.2, alpha = 0.22))
curve(dsn(x, omega = 0.2, alpha = 0.22))
curve(dsn(x, omega = 3, alpha = 0.22))
curve(dsn(x, omega = 8, alpha = 0.22))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(0,8))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(-8,8))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(0,8))
curve(dsn(x, omega = 8, alpha = 0.22), , ylim=c(0,0.8))
curve(dsn(x, omega = 8, alpha = 0.22))
curve(dsn(x, omega = 0.11, alpha = 0.22))
curve(dsn(x, omega = 0.21, alpha = 0.22))
curve(dsn(x, omega = 0.41, alpha = 0.22))
curve(dsn(x, omega = 0.1, alpha = 0.22))
curve(dsn(x, omega = 0.1, alpha = 2))
curve(dsn(x, omega = 0.1, alpha = 10))
curve(dsn(x, omega = 0.1, alpha = 100))
curve(dsn(x, omega = 0.1, alpha = 0.011))
curve(dst(x, omega = 0.1, alpha = 0.011))
curve(dst(x, omega = 1, alpha = 0.011))
curve(dst(x, omega = 1, alpha = 0.11))
curve(dst(x, omega = 1, alpha = 11))
curve(dst(x, omega = 0.9, alpha = 11))
curve(dst(x, omega = 0.11, alpha = 11))
curve(dst(x, omega = 0.11, alpha = 0.11))
rst(x, omega = 0.11, alpha = 0.11))
rst(100, omega = 0.11, alpha = 0.11))
rst(100, omega = 0.11, alpha = 0.11)
rst(100, omega = 0.2, alpha = 0.11, xi = 2)
rst(100, omega = 0.2, alpha = 0.11, xi = 0)
rst(100, omega = 0.2, alpha = 0.11, xi = 0, nu = 2)
sin(30)
sin(30°)
sin(30)
sin(pi/6)
tan(pi/6)
curve(sin(x))
curve(sin(x), xlim = c(-3,3))
curve(sin(x), xlim = c(-30,30))
curve(sin(x), xlim = c(-10,10))
line(h=0)
aline(h=0)
abline(h=0)
curve(tan(x), xlim = c(-10,10))
curve(tan(x), xlim = c(-10,10))
abline(h=0)
curve((x), xlim = c(-10,10))
abline(v=0)
curve((x), xlim = c(0,10))
abline(v=0)
abline(v=10)
curve((x), xlim = c(0,10))
abline(v=10)
atan(1)
atan(pi)
pi/4
tan(pi/3)
tan(2*pi/3)
fd = function(x, alpha){
disf = exp(-x^alpha)
}
fd(2, 1)
fd = function(x, alpha){
disf = exp(-x^alpha)
return(disf)
}
fd(2, 1)
fd(0.2, 1)
curve(fd(x, 1))
curve(fd(x, 1), xlim = c(-10,10))
fd = function(x, alpha){
disf = exp(-x^(-alpha))
return(disf)
}
curve(fd(x, 1), xlim = c(-10,10))
curve(fd(x, 1), xlim = c(0,10))
curve(fd(x, 1), xlim = c(0,4))
install.packages('Rtolls40')
install.packages('Rtolls')
library(installr)
updateR()
updateR()
install.packages('Rtolls')
install.packages(c("fGarch", "forecast", "installr", "quantmod", "rugarch", "timeSeries", "tseries", "vars", "xlsx"))
install.packages('Rtolls')
install.packages('Rtolls40')
install.packages('rtolls40')
curve(x^0.4)
curve(x^0.4, ylim = c(0, 4))
curve(x^0.4, ylim = c(0, 1))
curve(x^0.2, ylim = c(0, 1))
curve(x^0.1, ylim = c(0, 1))
curve(x^0.01, ylim = c(0, 1))
curve(x^0.001, ylim = c(0, 1))
curve(x^0.1, ylim = c(0, 1))
curve(x^1, ylim = c(0, 1))
curve(x^0.1, ylim = c(0, 1))
curve(x^0.2, ylim = c(0, 1))
curve(x^0.4, ylim = c(0, 1), add=T)
install.packages("knitr")
install.packages("readxl")
# Defining my work diretory
setwd("C:/Users/user/Downloads/ML_work/Algorithm")
library(readxl)
teste <- read_excel("teste.xlsx")
teste$foot = NULL
df2 = teste[,'sex']
teste[,'sex']=NULL
teste$sex = df2
teste = data.frame(teste)
#--- function
naive_marcos2 = function(k, df){
df = as.data.frame(df)
#fator =  factor(df[,k])
a = prop.table(table(df[ ,k]))
ta = length(a)
nm = rownames(a)
print('Marcos Naive Bayes Classifier for Discrete Predictors')
cat('A-priori probabilities:\n')
#df2 = df[ , k]
print(a)
#df[ ,k] = NULL
#col_n = colnames(df)
#df[,k] = df2
M = array(0, dim = c(2,2, ta))
m = matrix(0, 2, 2)
for(g in 1:ta){
m1 = as.matrix(tapply(df[,1], df[,k], mean)[g])
v1 = as.matrix(tapply(df[,1], df[,k], sd)[g])
m2 = tapply(df[,2], df[,k], mean)[g]
v2 = tapply(df[,2], df[,k], sd)[g]
m = matrix(c(m1, m2, v1, v2)  )
M[, ,g] = m
cat(nm[g], '\n')
print(M[, ,g])
}
return(M)
}
cc = naive_marcos2('sex', teste)
library(readxl)
teste <- read_excel("teste.xlsx")
setwd("C:/Users/user/Downloads/ML_work/Algorithm")
getwd()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "D:/Git projects/college_works/ML_1")
library(knitr)
library(kableExtra)
#options(kableExtra.latex.load_packages = FALSE)
library(magrittr)
knitr:: kable(a, booktabs = T) %>%kable_styling(full_width = T) %>%
column_spec(1, width = "8cm")
library(magrittr)
knitr::kable(a)
library(tinytex)
remove.packages("tinytex", lib="~/R/win-library/4.0")
install.packages('tinytex')
knit_with_parameters('C:/Users/user/Downloads/ML_work/testemarkdown/MJ_Ribeiro.Rmd')
library(tinytex)
library(tinytex)
2.8*4
1,2/11.2
1.2/11.2
setwd("D:/Git projects/college_works/eco_fin")
# functions
metrics = function(cm){
acurácia = (cm[["table"]][1,1] + cm[["table"]][2,2])/sum(cm[["table"]])
cpc = cm[["table"]][2,2] / ( cm[["table"]][1,2] + cm[["table"]][2,2] )
sensibilidade = cm[["table"]][1,1] / ( cm[["table"]][1,1] + cm[["table"]][2,1] )
especificidade = cm[["table"]][2,2] /( cm[["table"]][2,2] + cm[["table"]][1,2] )
G = sqrt(sensibilidade*especificidade)
LP = sensibilidade/(1 - especificidade)
LR = (1 - sensibilidade)/(especificidade)
DP = sqrt(pi)/3 * ( log(sensibilidade/(1 - sensibilidade) ) + log( especificidade/(1 - especificidade) )  )
gamma = sensibilidade - (1 - especificidade)
BA = (1/2) * (sensibilidade + especificidade)
métricas = data.frame(acurácia, cpc, sensibilidade, especificidade, G, LP, LR, DP, gamma, BA)
}
# libraries
library('xtable')
library(caret)
library(ROSE)
#--- load variables
df = readRDS('df.rds')
# df2 = df
#
# data = rownames(df2[2:242,])
#
# rgold = diff(log(df2$gold))
# roil = diff(log(df2$oil))
# rcdi = diff(log(df2$cdi))
# rcb = diff(log(df2$cb))
# rembi = diff(log(as.numeric(df2$embi)))
# crise = df2$crise[2:242]
# rvix =  df2$rvix[2:242]
# rav = df2$rav[2:242]
#
# dff = data.frame(rgold, roil, rcdi, rcb, rembi, rvix, rav, crise)
# rownames(dff) = data
#--- Rose library
library(ROSE)
df3 = ovun.sample(as.factor(crise)~., data=df, method="both", p=0.50,
subset=options("subset")$subset,
na.action=options("na.action")$na.action, seed=1)
df3 = data.frame(df3$data)
prop.table(table(df3$crise))
#---- Control train
control_train = trainControl(method = 'repeatedcv', number = 10, repeats = 2)    # ten fold
model_h =train(as.factor(crise) ~  gold + embi + oil + cb + cdi, data=df3,
trControl = control_train, method='xgbTree')
cm_rf1 = confusionMatrix(model_h)
confusionMatrix(model_h)
options(repos = getOption("repos")["CRAN"])
library('mxnet')
model_m = train(as.factor(crise) ~  rav, data=df3,
trControl = control_train,
method='glm',
family='binomial')
model_m
confusionMatrix(model_m)
warnings()
source('D:/Git projects/college_works/eco_fin/algoritms_rf_lg_RAV.R')
semrav
comrav
sorav
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='MLP', trControl=ctrl.mxnet,
tuneGrid=mxnet.params, num.round=20,
maximize=TRUE)
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
tuneGrid=mxnet.params, num.round=20,
maximize=TRUE)
mxnet.params <- expand.grid(layer1=192, layer2=96, layer3=10,
learning.rate=0.12, momentum=0.88,
dropout=0, repeats=1)
ctrl.mxnet <- trainControl(method='cv',
verboseIter=TRUE,
summaryFunction=multiClassSummary,
number = 10)
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
tuneGrid=mxnet.params, num.round=20,
maximize=TRUE)
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
num.round=20,
maximize=TRUE)
fit.mxnet
ctrl.mxnet <- trainControl(method='cv',
verboseIter=TRUE,
summaryFunction=multiClassSummary,
number = 100)
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
num.round=20,
maximize=TRUE)
mxnet.params <- expand.grid(size=5)
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
tuneGrid=mxnet.params, num.round=20,
maximize=TRUE)
ctrl.mxnet <- trainControl(method='cv',
verboseIter=TRUE,
summaryFunction=multiClassSummary,
number = 10)
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
tuneGrid=mxnet.params, num.round=20,
maximize=TRUE)
fit.mxnet
warnings()
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
tuneGrid=mxnet.params,
maximize=TRUE)
warnings()
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
maximize=TRUE)
fit.mxnet
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlpSGD', trControl=ctrl.mxnet,
maximize=TRUE)
install.packages('FCNN4R')
install.packages("C:/Users/user/Downloads/FCNN4R_0.3.0.tar.gz", repos = NULL, type = "source")
library('FCNN4R')
library('FCNN4R')
library(FCNN4R)
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlpWeightDecay', trControl=ctrl.mxnet,
maximize=TRUE)
fit.mxnet
confusionMatrix(fit.mxnet)
warnings()
mxnet.params <- expand.grid(layer1=10, layer2=10)
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
tuneGrid=mxnet.params,
maximize=TRUE)
mxnet.params <- expand.grid(decay=0.3)
fit.mxnet <- train(as.factor(crise) ~  gold + embi + oil + cb + rav + cdi, data=df3,
method='mlp', trControl=ctrl.mxnet,
tuneGrid=mxnet.params,
maximize=TRUE)
